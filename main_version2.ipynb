{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_version2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrma5ikgumRR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.utils.data as Data\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Initialize Params'''\n",
        "epochs = 100\n",
        "learning_rate = 0.03\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "hMakekbeuqKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/food.zip\""
      ],
      "metadata": {
        "id": "YoFJJ_Kout3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Load Data'''\n",
        "def readFile(path,label):\n",
        "    image_dir = sorted(os.listdir(path))\n",
        "    # x stores photos\n",
        "    x = np.zeros((len(image_dir),128,128,3),dtype=np.uint8)\n",
        "    # y stores labels\n",
        "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
        "    for i, file in enumerate(image_dir):\n",
        "        img = cv2.imread(os.path.join(path, file))\n",
        "        x[i, :, :] = cv2.resize(img,(128, 128))\n",
        "        if label:\n",
        "            y[i] = int(file.split(\"_\")[0])\n",
        "    if label:\n",
        "        return x,y\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "train_x, train_y = readFile('/content/food/training',True)\n",
        "val_x, val_y = readFile('/content/food/validation',True)\n",
        "test_x = readFile('/content/food/testing',False)\n",
        "print(\"Reading data: \")\n",
        "print(\"Size of training data = {}\".format(len(train_x)))\n",
        "print(\"Size of validation data = {}\".format(len(val_x)))\n",
        "print(\"Size of Testing data = {}\".format(len(test_x)))"
      ],
      "metadata": {
        "id": "Ud7EjomBuvJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = torch.tensor(train_x)\n",
        "# print(train_x.shape)  -> torch.Size([9866, 3, 128, 128])\n",
        "train_x = train_x.transpose(1,3).float()\n",
        "# print(train_x.shape) -> torch.Size([9866, 128, 128, 3])\n",
        "train_y = torch.tensor(train_y)\n",
        "val_x = torch.tensor(val_x)\n",
        "val_x = val_x.transpose(1, 3).float()\n",
        "val_y = torch.tensor(val_y)\n",
        "\n",
        "train_dataset = Data.TensorDataset(train_x,train_y)\n",
        "val_dataset = Data.TensorDataset(val_x,val_y)\n",
        "\n",
        "train_loader = Data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "val_loader = Data.DataLoader(dataset=val_dataset,batch_size=batch_size,shuffle=True) "
      ],
      "metadata": {
        "id": "UKABGPTVuxta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        #torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
        "        #input 维度 [3, 128, 128]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),  # 输出[64, 128, 128]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # 输出[64, 64, 64]\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), # 输出[128, 64, 64]\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # 输出[128, 32, 32]\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), # 输出[256, 32, 32]\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # 输出[256, 16, 16] \n",
        " \n",
        "        )\n",
        "        # 全连接的前向传播神经网络\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256*16*16, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 11)   # 最后是11个分类\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.cnn(x)\n",
        "        out = out.view(out.size()[0], -1)  # 摊平成1维\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "kQmalK1fu0Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Initialize the network'''\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "_CNBrndPu6ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print(\"CUDA is available! Training on GPU...\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Training on CPU...\")"
      ],
      "metadata": {
        "id": "OWCyjkRSu_JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().cuda() #用cuda加速\n",
        "loss = nn.CrossEntropyLoss() # 因为是分类任务，所以使用交叉熵损失 \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # 使用Adam优化器\n",
        "num_epoch = 30 #迭代次数\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    epoch_start_time = time.time()\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    model.train() # 确保 model 是在 训练 model (开启 Dropout 等...)\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad() # 用 optimizer 将模型参数的梯度 gradient 归零\n",
        "        train_pred = model(data[0].cuda()) # 利用 model 得到预测的概率分布，这边实际上是调用模型的 forward 函数\n",
        "        batch_loss = loss(train_pred, data[1].cuda()) # 计算 loss （注意 prediction 跟 label 必须同时在 CPU 或是 GPU 上）\n",
        "        batch_loss.backward() # 利用 back propagation 算出每个参数的 gradient\n",
        "        optimizer.step() # 以 optimizer 用 gradient 更新参数\n",
        "\n",
        "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
        "        train_loss += batch_loss.item()\n",
        "    \n",
        "    #验证集val\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            val_pred = model(data[0].cuda())\n",
        "            batch_loss = loss(val_pred, data[1].cuda())\n",
        "\n",
        "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
        "            val_loss += batch_loss.item()\n",
        "\n",
        "        #将结果 print 出來\n",
        "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
        "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
        "             train_acc/train_dataset.__len__(), train_loss/train_dataset.__len__(), val_acc/val_dataset.__len__(), val_loss/val_dataset.__len__()))"
      ],
      "metadata": {
        "id": "dKVgnVkqvAsh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}